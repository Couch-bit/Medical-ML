{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42712636-af0d-4298-b64c-f171216c91b4",
   "metadata": {},
   "source": [
    "# Medical-ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a6f134-a556-486b-97f5-8789a5313100",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e639493-1945-4d1b-b9de-7ba938a90a6d",
   "metadata": {},
   "source": [
    "This project aims to explore and analyze heart failure data to develop and evaluate machine learning models capable of delivering accurate predictions.\n",
    "\n",
    "The dataset analyzed in this project focuses on heart failure, specifically determining whether a patient has heart disease based on various demographic characteristics and biomarkers. Originally titled *Heart Failure Prediction*, the dataset was sourced from [Kaggle](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction?select=heart.csv).\n",
    "\n",
    "**The introductory part contains setting up the environment, and function definitions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42524b16-439b-4d00-aada-f9705733e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dalex as dx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import warnings\n",
    "\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.pipeline import Pipeline\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats.contingency import association\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, make_scorer\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    train_test_split,\n",
    "    TunedThresholdClassifierCV,\n",
    ")\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e78b0f-d352-4ed6-8c8c-caa289cfc10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "sns.set_theme(style='whitegrid')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686a245c-a9db-4380-bd25-27cade365268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An improvement upon KNNImputer found in sklearn.\n",
    "class BetterKNNImputer(KNNImputer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        numeric_cols: list,\n",
    "        categorical_cols: list,\n",
    "        target_col: str,\n",
    "        is_target_numeric=True,\n",
    "    ):\n",
    "        self.numeric_cols = [col for col in numeric_cols if col != target_col]\n",
    "        self.categorical_cols = [col for col in categorical_cols if col != target_col]\n",
    "        self.target_col = target_col\n",
    "        self.is_target_numeric = is_target_numeric\n",
    "        \n",
    "        self.numeric_transformer = StandardScaler()\n",
    "        self.categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "        \n",
    "        if is_target_numeric:\n",
    "            self.target_transformer = StandardScaler()\n",
    "            self.undersampler = None\n",
    "        else:\n",
    "            self.target_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "            # NearMiss-1 seems to do well here.\n",
    "            self.undersampler = NearMiss()\n",
    "            \n",
    "        super().__init__()\n",
    "        self.weights = 'distance'\n",
    "\n",
    "    def _preprocess_data_fit(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "    ) -> tuple[pd.DataFrame, pd.DataFrame, np.ndarray, np.ndarray]:\n",
    "        # Extract the column of interest from X.\n",
    "        target = X[[self.target_col]].copy()\n",
    "        X = X.drop(self.target_col, axis=1)\n",
    "        \n",
    "        # Standardize numeric columns.\n",
    "        X[self.numeric_cols] = self.numeric_transformer.fit_transform(\n",
    "            X[self.numeric_cols]\n",
    "        )\n",
    "        \n",
    "        # Perform One Hot Encoding on categorical variables.\n",
    "        categorical_dummies = self.categorical_transformer.fit_transform(\n",
    "            X[self.categorical_cols]\n",
    "        )\n",
    "        categorical_features = self.categorical_transformer.get_feature_names_out()\n",
    "        categorical_dummies = pd.DataFrame(\n",
    "            data=categorical_dummies.toarray(),\n",
    "            columns=categorical_features,\n",
    "        )\n",
    "        \n",
    "        categorical_features = [\n",
    "            col for col in categorical_features if not col.endswith('nan')\n",
    "        ]\n",
    "        categorical_dummies = categorical_dummies[categorical_features]\n",
    "        \n",
    "        # Concat X so everything in it is encoded.\n",
    "        X = X.drop(self.categorical_cols, axis=1)\n",
    "        X = pd.concat([X.reset_index(drop=True), categorical_dummies], axis=1)\n",
    "\n",
    "        # Perform undersampling if target variable is categorical.\n",
    "        X_mask = ~X.isna().any(axis=1).to_numpy()\n",
    "        old_target_mask = ~target.isna().any(axis=1).to_numpy()\n",
    "        X_target_mask = X_mask & old_target_mask\n",
    "        if not self.is_target_numeric:\n",
    "            X_undersampled, target_undersampled = self.undersampler.fit_resample(\n",
    "                X[X_target_mask],\n",
    "                target[X_target_mask],\n",
    "            )\n",
    "            X = pd.concat([X[~X_target_mask], X_undersampled])\n",
    "            target = pd.concat([target[~X_target_mask], target_undersampled])\n",
    "\n",
    "        # Make sure index for X isn't nonsense.\n",
    "        X = X.reset_index(drop=True)\n",
    "\n",
    "        # Mask is useful for fitting.\n",
    "        new_target_mask = ~target.isna().any(axis=1).to_numpy()\n",
    "        # Transform the target variable.\n",
    "        target = self.target_transformer.fit_transform(target)\n",
    "        \n",
    "        return X, target, old_target_mask, new_target_mask\n",
    "\n",
    "    def _preprocess_data_transform(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "    ) -> tuple[pd.DataFrame, pd.DataFrame, np.ndarray]:\n",
    "        # Extract the column of interest from X.\n",
    "        target = X[[self.target_col]].copy()\n",
    "        X = X.drop(self.target_col, axis=1)\n",
    "\n",
    "        # Standardize numeric columns.\n",
    "        X[self.numeric_cols] = self.numeric_transformer.transform(\n",
    "            X[self.numeric_cols]\n",
    "        )\n",
    "\n",
    "        # Perform One Hot Encoding on categorical variables.\n",
    "        categorical_dummies = self.categorical_transformer.transform(\n",
    "            X[self.categorical_cols]\n",
    "        )\n",
    "        categorical_features = self.categorical_transformer.get_feature_names_out()\n",
    "        categorical_dummies = pd.DataFrame(\n",
    "            data=categorical_dummies.toarray(),\n",
    "            columns=categorical_features,\n",
    "        )\n",
    "        categorical_features = [\n",
    "            col for col in categorical_features if not col.endswith('nan')\n",
    "        ]\n",
    "        categorical_dummies = categorical_dummies[categorical_features]\n",
    "\n",
    "        # Concat X so everything in it is encoded.\n",
    "        X = X.drop(self.categorical_cols, axis=1)\n",
    "        X = pd.concat([X.reset_index(drop=True), categorical_dummies], axis=1)\n",
    "\n",
    "        # Mask is useful for transforming.\n",
    "        target_mask = ~target.isna().any(axis=1).to_numpy()\n",
    "        # Transform the target variable.\n",
    "        target = self.target_transformer.transform(target)\n",
    "        \n",
    "        return X, target, target_mask\n",
    "\n",
    "    def fit(self, X: pd.DataFrame):\n",
    "        new_data = X.copy()\n",
    "        X, target, old_target_mask, new_target_mask = self._preprocess_data_fit(X)\n",
    "\n",
    "        if self.is_target_numeric:\n",
    "            target = pd.DataFrame(data=target, columns=[self.target_col])\n",
    "            X = pd.concat([X, target], axis=1)\n",
    "        else:\n",
    "            target_features = self.target_transformer.get_feature_names_out()\n",
    "            target = pd.DataFrame(data=target.toarray(),columns=target_features)\n",
    "            target_features = [\n",
    "                col for col in target_features if not col.endswith('nan')\n",
    "            ]\n",
    "            target = target[target_features]\n",
    "            # Makes it so missing columns are actually read as such.\n",
    "            target[~new_target_mask] = np.nan\n",
    "            X = pd.concat([X, target], axis=1)\n",
    "\n",
    "        # Set n_neighbors using a heuristic.\n",
    "        self.n_neighbors = int(len(target) ** 0.5)\n",
    "        \n",
    "        return super().fit(X)\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        new_data = X.copy()\n",
    "        X, target, target_mask = self._preprocess_data_transform(X)\n",
    "\n",
    "        if self.is_target_numeric:\n",
    "            target = pd.DataFrame(data=target, columns=[self.target_col])\n",
    "            X = pd.concat([X, target], axis=1)\n",
    "            new_target = super().transform(X)[:, [-1]]\n",
    "            new_target = self.target_transformer.inverse_transform(new_target)[:, 0]\n",
    "            new_data[self.target_col] = new_target\n",
    "        else:\n",
    "            target_features = self.target_transformer.get_feature_names_out()\n",
    "            target = pd.DataFrame(data=target.toarray(), columns=target_features)\n",
    "            target_features = [\n",
    "                col for col in target_features if not col.endswith('nan')\n",
    "            ]\n",
    "            target = target[target_features]\n",
    "            # Makes it so missing columns are actually read as such.\n",
    "            target[~target_mask] = np.nan\n",
    "            \n",
    "            X = pd.concat([X, target], axis=1)\n",
    "            new_target = super().transform(X)[:, (-len(target_features)):]\n",
    "            new_target = self.target_transformer.categories_[0][\n",
    "            np.argmax(new_target, axis=1)\n",
    "            ]\n",
    "            new_data[self.target_col] = new_target\n",
    "        \n",
    "        return new_data\n",
    "\n",
    "def cramerv(var1: np.ndarray, var2: np.ndarray) -> float:\n",
    "    crosstab = np.array(pd.crosstab(var1,var2, rownames=None, colnames=None))\n",
    "    return association(crosstab)\n",
    "\n",
    "def diagnostic_odds_ratio(ground_truth: np.ndarray, predictions: np.ndarray) -> float:\n",
    "    tn, fp, fn, tp = confusion_matrix(ground_truth, predictions).ravel()\n",
    "    if fp == 0 and fn == 0:\n",
    "        return tp * tn\n",
    "    elif fp == 0 or fn == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (tp * tn) / (fp * fn)\n",
    "\n",
    "diagnostic_odds_ratio_scorer = make_scorer(\n",
    "    diagnostic_odds_ratio,\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98978550-2f3c-45e3-b906-f8fd070008c1",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1281da98-8462-41fb-bb9a-0dded2d96682",
   "metadata": {},
   "source": [
    "The **Exploratory Data Analysis** seeks to uncover the underlying structure of the data, verify its suitability for machine learning, and ensure there are no missing values, inbalances, or structural issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b35cf88-f090-4dfe-86ea-dd097c65be1e",
   "metadata": {},
   "source": [
    "### Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94542ac-555f-44bd-8b54-4ac2e6616ff7",
   "metadata": {},
   "source": [
    "**The head of data is presented below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a2f35-c8f1-4afa-b8b3-be367c0df2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('heart_failure.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2678024-cb2d-49c4-9f15-765b9cbcc34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['FastingBS'] = data['FastingBS'].astype(str)\n",
    "numeric_cols = [\n",
    "    'Age',\n",
    "    'RestingBP',\n",
    "    'Cholesterol',\n",
    "    'MaxHR',\n",
    "    'Oldpeak',\n",
    "]\n",
    "categorical_cols = [\n",
    "    'Sex',\n",
    "    'ChestPainType',\n",
    "    'FastingBS',\n",
    "    'RestingECG',\n",
    "    'ExerciseAngina',\n",
    "    'ST_Slope',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d89f1d9-8eba-4520-b5b7-f276926378f0",
   "metadata": {},
   "source": [
    "**Verifying if there are missing values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1760819b-256c-4201-a769-7ac858cde372",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(data.isna().sum(), columns = ['missing']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581254dc-fa03-4744-bbfd-ad923afc624e",
   "metadata": {},
   "source": [
    "**There aren't any missing values in the data.** \n",
    "\n",
    "It's useful to review summary statistics for numeric columns to identify any unusual or illogical values. Those values may not only be outliers, but, in extreme cases, could indicate errors in the dataset (e.g., an age of 167)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1f41cf-1f4d-458a-b1de-a37e843efc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e175964c-9539-42ab-9f73-dae8b0d48eee",
   "metadata": {},
   "source": [
    "Only *Age* and *MaxHR* seem to have reasonable values. Other variables seem to have a few potential outliers - thus, boxplots will be used to check for any isolated extremes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2507f0e8-ac66-4f8c-a507-729270ad6a2a",
   "metadata": {},
   "source": [
    "### Distributions of numeric variables (boxplots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66430fcd-85d7-450b-a752-485f553cfc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data['Age'])\n",
    "_ = ax.set(title='Box Plot of Age', ylabel=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a6a111-4400-4f11-b584-902a389458d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data['RestingBP'])\n",
    "_ = ax.set(title='Box Plot of Resting Blood Pressure', ylabel=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d82ac-45c8-4ff1-b457-749cfbd0f9c0",
   "metadata": {},
   "source": [
    "It seems there are a few extreme values for *Resting Blood Pressure*. **However, these are likely not a result of a measurement error, but a fat-tailed distribution, and should be kept in the dataset.**\n",
    "\n",
    "A resting blood pressure of 80mmHg is low and could indicate hypotension, whereas 200mmHg  signals severe hypertension (it is extremely high but possible to attain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d776db07-ecd9-4cde-b8e1-8272e22f0bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data['Cholesterol'])\n",
    "_ = ax.set(title='Box Plot of Cholesterol', ylabel=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0005383c-ca4e-4819-b4d8-f11290a2ab89",
   "metadata": {},
   "source": [
    "**For *Cholesterol* a few observations should be removed or their value should be imputed.** In particular, a human cannot have 0 cholesterol, and the value above 500 warrants removal - such a case is extremely rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db1c296-9030-470a-8ec8-491a7c13827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data['MaxHR'])\n",
    "_ = ax.set(title='Box Plot of Max Heart Rate', ylabel=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bddec9-009f-43a0-ba23-a416e842b136",
   "metadata": {},
   "source": [
    "There are a few extreme values for *Max Heart Rate*. **However, these are likely not a result of a measurement error and should be kept in the dataset.**\n",
    "\n",
    "A maximum heart rate below 80 is highly unusual; however it is possible and may indicate severe cardiovascular issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270d8124-121b-4cb7-a4d2-52d35a460fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data['Oldpeak'])\n",
    "_ = ax.set(title='Box Plot of ST Depression', ylabel=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a955f82e-2b5d-4630-9515-f50d6ecef60c",
   "metadata": {},
   "source": [
    "There are a few outliers for *ST Depression*; however, **they remain within the realistic range** - a severe depression can oscillate around 5 mm and may indicate myocardial ischemia or other cardiac issues.\n",
    "\n",
    "For *Cholesterol* observations above 500 will be removed, and observations with 0 will be replaced through *KNN Inputer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c0881d-2728-4b00-bf52-ef8b20ae16a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['Cholesterol'] < 500]\n",
    "data.loc[data['Cholesterol'] == 0, 'Cholesterol'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0869f29c-8bd4-4602-a71a-e76a60403a99",
   "metadata": {},
   "source": [
    "### Relationships between numeric variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fb976-2319-4e4b-a65b-5bfe7b269e3a",
   "metadata": {},
   "source": [
    "**Now collinearity will be checked.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2a08d4-b775-44d8-9f21-bc97bb469f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[numeric_cols].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16128df6-d0d5-4189-97d4-a3b7c09974e6",
   "metadata": {},
   "source": [
    "There doesn't seem to be a problem with collinearity, as all the values are within a proper range.\n",
    "\n",
    "**Now the monotonic relationship between the numeric variables and the response variable will be checked via Spearman's $\\rho$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452661dc-30db-49ad-9663-4e7b049a992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_predictive_power = pd.DataFrame(columns = ['rho'])\n",
    "for col in numeric_cols:\n",
    "    temp_data = data[~data[col].isna()]\n",
    "    numeric_predictive_power.loc[col, :] = (\n",
    "        [spearmanr(temp_data[col], temp_data['HeartDisease']).statistic]\n",
    "    )\n",
    "numeric_predictive_power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be43dc0-17d9-4aa2-9583-5d9a0981c953",
   "metadata": {},
   "source": [
    "**It seems that *RestingBP* and *Cholesterol* may not have that much of an effect on the risk of heart disease**, which is surprising, considering that cholesterol is commonly associated with it. However, a low correlation does not imply that the variables are not significant once models are built."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4805c6-276f-4599-a83c-5bfbf6cdbba9",
   "metadata": {},
   "source": [
    "### Relationships between categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e35561e-6009-4466-9fc4-7e53a2506c14",
   "metadata": {},
   "source": [
    "Now categorical variables will be assessed. First, it will be verified if there are any extremely rare categories (< 1%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3e842c-2216-4484-9c88-111395be91d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    display(pd.DataFrame(data[col].value_counts(normalize=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387d360-9659-4401-9f8f-836bb93b5ece",
   "metadata": {},
   "source": [
    "The proportions fall within an acceptable range.\n",
    "\n",
    "**The dependency between categorical variables and the response variable will be checked using *Cramer's V*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bbbf53-a939-4a41-bed1-db39dc105ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_predictive_power = pd.DataFrame(columns = ['cramer_V'])\n",
    "for col in categorical_cols:\n",
    "    categorical_predictive_power.loc[col, :] = (\n",
    "        [cramerv(data[col], data['HeartDisease'])]\n",
    "    )\n",
    "categorical_predictive_power"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432eb238-e5cc-4a47-994b-6b05d10e6aac",
   "metadata": {},
   "source": [
    "Most of these variables seem to have some predictive power, but a possible exception is *RestingECG*. \n",
    "\n",
    "**The class imbalance in the response variable should also be checked.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b9f866-914f-4054-9c34-cb41d9784eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    'Proportion of people diagnosed with heart disease:',\n",
    "    data['HeartDisease'].sum() / len(data)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342804d1-74ea-4e14-af25-c0b834bd74c1",
   "metadata": {},
   "source": [
    "While the imbalance isn't extreme, it's worth considering during model training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88911025-3bfb-4aff-8760-81c36d9a6b1e",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06219e98-ac98-48ae-893c-a9c93e41d647",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44b74db-7df0-448f-9f84-1832ce3086fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('HeartDisease', axis=1)\n",
    "y = data['HeartDisease']\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa21c5-a006-4d1e-917c-92cb147f5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = BetterKNNImputer(\n",
    "    numeric_cols,\n",
    "    categorical_cols,\n",
    "    'Cholesterol',\n",
    "    is_target_numeric=True,\n",
    ")\n",
    "X_train['Cholesterol'] = imputer.fit_transform(X_train)['Cholesterol']\n",
    "X_test['Cholesterol'] = imputer.transform(X_test)['Cholesterol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593a950-33a4-45dd-a4d5-83349591825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3669493e-fb05-4249-ba31-5de3db8128cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[numeric_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115afc48-420a-4973-8c18-75e58579a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    display(pd.DataFrame(X_train[col].value_counts(normalize=True)))\n",
    "    display(pd.DataFrame(X_test[col].value_counts(normalize=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b81577f-5631-4259-ad65-a2c3736671b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train proportion:', y_train.sum() / len(y_train))\n",
    "print('Test proportion:', y_test.sum() / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9b3260-360a-454f-86de-480dc17fa6d4",
   "metadata": {},
   "source": [
    "Split seems fairly good. It's time to try building the some models. We're going to use **AP (Average Precision, threshold-agnostic metric used for tuning hiperparameters)** and **DOR (Diagnostic Odds Ratio, used for tuning the threshold and SVM)** for cross-validation.  We will test a few potential models:\n",
    "- logistic regression,\n",
    "- random forest,\n",
    "- decision tree boosting,\n",
    "- SVM.\n",
    "\n",
    "First we will simply use the dataset without any modifications, then apply observation weighting, and afterwards we will also attempt to use undersampling/oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987a2f19-5e11-4a86-bcd0-87dc9710291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_thresholds = np.linspace(0.01, 0.5, 201)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2373a110-cbf0-49d1-b8b8-1053601fc307",
   "metadata": {},
   "source": [
    "### No Weighting or Under-/Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25599c01-8ba8-467e-a868-0b427515d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols),\n",
    "    ]\n",
    ")\n",
    "logistic_pipeline = Pipeline(\n",
    "    [\n",
    "        ('transformer', transformer),\n",
    "        ('model', LogisticRegression(penalty=None)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "threshold_tuning = TunedThresholdClassifierCV(\n",
    "    logistic_pipeline,\n",
    "    scoring=diagnostic_odds_ratio_scorer,\n",
    "    thresholds=valid_thresholds,\n",
    "    cv=10,\n",
    ")\n",
    "threshold_tuning.fit(X_train, y_train)\n",
    "logistic_pipeline = threshold_tuning.estimator_\n",
    "logistic_threshold = threshold_tuning.best_threshold_\n",
    "display(logistic_pipeline)\n",
    "print('Best threshold:', logistic_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b993d174-1277-4168-a61a-5673c25d64d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ColumnTransformer(\n",
    "    [('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)],\n",
    "    remainder='passthrough',\n",
    "    force_int_remainder_cols=False,\n",
    ")\n",
    "forest_pipeline = Pipeline(\n",
    "    [\n",
    "        ('transformer', transformer),\n",
    "        ('model', RandomForestClassifier()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    'model__n_estimators': [100, 150, 200, 250],\n",
    "    'model__criterion': ['gini', 'entropy'],\n",
    "    'model__max_features': [0.1, 0.2, 0.3, 0.4],\n",
    "    'model__max_depth': [2, 3, 5],\n",
    "}\n",
    "parameter_tuning = RandomizedSearchCV(\n",
    "    forest_pipeline,\n",
    "    parameters,\n",
    "    n_iter=50,\n",
    "    scoring='average_precision',\n",
    "    cv=10,\n",
    ")\n",
    "parameter_tuning.fit(X_train, y_train)\n",
    "forest_pipeline = parameter_tuning.best_estimator_\n",
    "display(forest_pipeline)\n",
    "\n",
    "threshold_tuning = TunedThresholdClassifierCV(\n",
    "    forest_pipeline,\n",
    "    scoring=diagnostic_odds_ratio_scorer,\n",
    "    thresholds=valid_thresholds,\n",
    "    cv=10,\n",
    ")\n",
    "threshold_tuning.fit(X_train, y_train)\n",
    "forest_threshold = threshold_tuning.best_threshold_\n",
    "print('Best threshold:', forest_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95abdfb-7bff-4caa-916e-2be348bdd4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ColumnTransformer(\n",
    "    [('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)],\n",
    "    remainder='passthrough',\n",
    "    force_int_remainder_cols=False,\n",
    ")\n",
    "\n",
    "boost_pipeline = Pipeline(\n",
    "    [\n",
    "        ('transformer', transformer),\n",
    "        ('model', AdaBoostClassifier(algorithm='SAMME')),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    'model__n_estimators': [5, 10, 15, 20],\n",
    "    'model__learning_rate': [0.4, 0.8, 1.2, 1.6, 2],\n",
    "    'model__estimator': [\n",
    "        DecisionTreeClassifier(\n",
    "            max_depth=1,\n",
    "            criterion=crit)\n",
    "        for crit in ['gini', 'entropy']\n",
    "    ],\n",
    "}\n",
    "parameter_tuning = GridSearchCV(\n",
    "    boost_pipeline,\n",
    "    parameters,\n",
    "    scoring='average_precision',\n",
    "    cv=10,\n",
    ")\n",
    "parameter_tuning.fit(X_train, y_train)\n",
    "boost_pipeline = parameter_tuning.best_estimator_\n",
    "display(boost_pipeline)\n",
    "\n",
    "threshold_tuning = TunedThresholdClassifierCV(\n",
    "    boost_pipeline,\n",
    "    scoring=diagnostic_odds_ratio_scorer,\n",
    "    thresholds=valid_thresholds,\n",
    "    cv=10,\n",
    ")\n",
    "threshold_tuning.fit(X_train, y_train)\n",
    "boost_threshold = threshold_tuning.best_threshold_\n",
    "print('Best threshold:', boost_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000e574a-5590-4312-b867-791e84f33430",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "svm_pipeline = Pipeline(\n",
    "    [\n",
    "        ('transformer', transformer),\n",
    "        ('model', SVC()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = [\n",
    "    {\n",
    "        'model__kernel': ['linear'],\n",
    "        'model__C': [2 ** (i - 9) for i in range(19)],\n",
    "    },\n",
    "    {\n",
    "        'model__kernel': ['poly'],\n",
    "        'model__C': [2 ** (i - 9) for i in range(10)],\n",
    "        'model__degree': [i + 2 for i in range(4)],\n",
    "        'model__coef0': [0, 0.1, 0.5, 1, 2, 10],\n",
    "    },\n",
    "    {\n",
    "        'model__kernel': ['rbf'],\n",
    "        'model__C': [2 ** (i - 9) for i in range(10)],\n",
    "        'model__gamma': [2 ** (i - 8) for i in range(17)],\n",
    "    },\n",
    "]\n",
    "parameter_tuning = RandomizedSearchCV(\n",
    "    svm_pipeline,\n",
    "    parameters,\n",
    "    n_iter=50,\n",
    "    scoring=diagnostic_odds_ratio_scorer,\n",
    "    cv=10,\n",
    ")\n",
    "parameter_tuning.fit(X_train, y_train)\n",
    "svm_pipeline = parameter_tuning.best_estimator_\n",
    "display(svm_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c2fa08-dd1a-43bc-8b95-0da566311293",
   "metadata": {},
   "source": [
    "### Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b1e0ca-9ee7-4670-a83f-cc85aa82b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols),\n",
    "    ]\n",
    ")\n",
    "logistic_pipeline_weighted = Pipeline(\n",
    "    [\n",
    "        ('transformer', transformer),\n",
    "        ('model', LogisticRegression(penalty=None, class_weight='balanced')),\n",
    "    ]\n",
    ")\n",
    "\n",
    "threshold_tuning = TunedThresholdClassifierCV(\n",
    "    logistic_pipeline_weighted,\n",
    "    scoring=diagnostic_odds_ratio_scorer,\n",
    "    thresholds=valid_thresholds,\n",
    "    cv=10,\n",
    ")\n",
    "threshold_tuning.fit(X_train, y_train)\n",
    "logistic_pipeline_weighted = threshold_tuning.estimator_\n",
    "logistic_threshold_weighted = threshold_tuning.best_threshold_\n",
    "display(logistic_pipeline_weighted)\n",
    "print('Best threshold:', logistic_threshold_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8990c2-aed0-4ac6-888d-2d0cd9316fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ColumnTransformer(\n",
    "    [('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)],\n",
    "    remainder='passthrough',\n",
    "    force_int_remainder_cols=False,\n",
    ")\n",
    "forest_pipeline_weighted = Pipeline(\n",
    "    [\n",
    "        ('transformer', transformer),\n",
    "        ('model', RandomForestClassifier(class_weight='balanced')),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    'model__n_estimators': [100, 150, 200, 250],\n",
    "    'model__criterion': ['gini', 'entropy'],\n",
    "    'model__max_features': [0.1, 0.2, 0.3, 0.4],\n",
    "    'model__max_depth': [2, 3, 5],\n",
    "}\n",
    "parameter_tuning = RandomizedSearchCV(\n",
    "    forest_pipeline_weighted,\n",
    "    parameters,\n",
    "    n_iter=50,\n",
    "    scoring='average_precision',\n",
    "    cv=10,\n",
    ")\n",
    "parameter_tuning.fit(X_train, y_train)\n",
    "forest_pipeline_weighted = parameter_tuning.best_estimator_\n",
    "display(forest_pipeline_weighted)\n",
    "\n",
    "threshold_tuning = TunedThresholdClassifierCV(\n",
    "    forest_pipeline_weighted,\n",
    "    scoring=diagnostic_odds_ratio_scorer,\n",
    "    thresholds=valid_thresholds,\n",
    "    cv=10,\n",
    ")\n",
    "threshold_tuning.fit(X_train, y_train)\n",
    "forest_threshold_weighted = threshold_tuning.best_threshold_\n",
    "print('Best threshold:', forest_threshold_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa7d45-948d-4b18-a79c-cecf982dbff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ColumnTransformer(\n",
    "    [('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)],\n",
    "    remainder='passthrough',\n",
    "    force_int_remainder_cols=False,\n",
    ")\n",
    "\n",
    "boost_pipeline_weighted = Pipeline(\n",
    "    [\n",
    "        ('transformer', transformer),\n",
    "        ('model', AdaBoostClassifier(algorithm='SAMME')),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    'model__n_estimators': [5, 10, 15, 20],\n",
    "    'model__learning_rate': [0.4, 0.8, 1.2, 1.6, 2],\n",
    "    'model__estimator': [\n",
    "        DecisionTreeClassifier(\n",
    "            max_depth=1,\n",
    "            criterion=crit,\n",
    "            class_weight='balanced')\n",
    "        for crit in ['gini', 'entropy']\n",
    "    ],\n",
    "}\n",
    "parameter_tuning = GridSearchCV(\n",
    "    boost_pipeline_weighted,\n",
    "    parameters,\n",
    "    scoring='average_precision',\n",
    "    cv=10,\n",
    ")\n",
    "parameter_tuning.fit(X_train, y_train)\n",
    "boost_pipeline_weighted = parameter_tuning.best_estimator_\n",
    "display(boost_pipeline_weighted)\n",
    "\n",
    "threshold_tuning = TunedThresholdClassifierCV(\n",
    "    boost_pipeline_weighted,\n",
    "    scoring=diagnostic_odds_ratio_scorer,\n",
    "    thresholds=valid_thresholds,\n",
    "    cv=10,\n",
    ")\n",
    "threshold_tuning.fit(X_train, y_train)\n",
    "boost_threshold_weighted = threshold_tuning.best_threshold_\n",
    "print('Best threshold:', boost_threshold_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e48ec3a-277d-413f-b2bc-d86de439ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "svm_pipeline_weighted = Pipeline(\n",
    "    [\n",
    "        ('transformer', transformer),\n",
    "        ('model', SVC(class_weight='balanced')),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = [\n",
    "    {\n",
    "        'model__kernel': ['linear'],\n",
    "        'model__C': [2 ** (i - 9) for i in range(19)],\n",
    "    },\n",
    "    {\n",
    "        'model__kernel': ['poly'],\n",
    "        'model__C': [2 ** (i - 9) for i in range(10)],\n",
    "        'model__degree': [i + 2 for i in range(4)],\n",
    "        'model__coef0': [0, 0.1, 0.5, 1, 2, 10],\n",
    "    },\n",
    "    {\n",
    "        'model__kernel': ['rbf'],\n",
    "        'model__C': [2 ** (i - 9) for i in range(10)],\n",
    "        'model__gamma': [2 ** (i - 8) for i in range(17)],\n",
    "    },\n",
    "]\n",
    "parameter_tuning = RandomizedSearchCV(\n",
    "    svm_pipeline_weighted,\n",
    "    parameters,\n",
    "    n_iter=50,\n",
    "    scoring=diagnostic_odds_ratio_scorer,\n",
    "    cv=10,\n",
    ")\n",
    "parameter_tuning.fit(X_train, y_train)\n",
    "svm_pipeline_weighted = parameter_tuning.best_estimator_\n",
    "display(svm_pipeline_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012e4c3f-1f8c-4253-b636-2075449ea5d2",
   "metadata": {},
   "source": [
    "### Undersampling\n",
    "We will use NearMiss undersampling. Each of the three variants will be tested during cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c43bb30-fd50-43b5-846e-6a1e85806b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_cols),\n",
    "    ]\n",
    ")\n",
    "logistic_pipeline_undersampled = Pipeline(\n",
    "    [\n",
    "        ('transformer', transformer),\n",
    "        ('undersampler', NearMiss()),\n",
    "        ('model', LogisticRegression(penalty=None)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = {'undersampler__version': [1, 2, 3]}\n",
    "parameter_tuning = GridSearchCV(\n",
    "    logistic_pipeline_undersampled,\n",
    "    parameters,\n",
    "    scoring='average_precision',\n",
    "    cv=10,\n",
    ")\n",
    "parameter_tuning.fit(X_train, y_train)\n",
    "logistic_pipeline_undersampled = parameter_tuning.best_estimator_\n",
    "display(logistic_pipeline_undersampled)\n",
    "\n",
    "threshold_tuning = TunedThresholdClassifierCV(\n",
    "    logistic_pipeline_undersampled,\n",
    "    scoring=diagnostic_odds_ratio_scorer,\n",
    "    thresholds=valid_thresholds,\n",
    "    cv=10,\n",
    ")\n",
    "threshold_tuning.fit(X_train, y_train)\n",
    "logistic_threshold_undersampled = threshold_tuning.best_threshold_\n",
    "print('Best threshold:', logistic_threshold_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269da4c6-97a8-4b27-9124-30ad2089af57",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "    ]\n",
    ")\n",
    "forest_pipeline_undersampled = Pipeline(\n",
    "    [\n",
    "        ('transformer', transformer),\n",
    "        ('undersampler', NearMiss()),\n",
    "        ('model', RandomForestClassifier()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    'undersampler__version': [1, 2, 3],\n",
    "    'model__n_estimators': [100, 150, 200, 250],\n",
    "    'model__criterion': ['gini', 'entropy'],\n",
    "    'model__max_features': [0.1, 0.2, 0.3, 0.4],\n",
    "    'model__max_depth': [2, 3, 5],\n",
    "}\n",
    "parameter_tuning = RandomizedSearchCV(\n",
    "    forest_pipeline_undersampled,\n",
    "    parameters,\n",
    "    n_iter=50,\n",
    "    scoring='average_precision',\n",
    "    cv=10,\n",
    ")\n",
    "parameter_tuning.fit(X_train, y_train)\n",
    "forest_pipeline_undersampled = parameter_tuning.best_estimator_\n",
    "display(forest_pipeline_undersampled)\n",
    "\n",
    "threshold_tuning = TunedThresholdClassifierCV(\n",
    "    forest_pipeline_undersampled,\n",
    "    scoring=diagnostic_odds_ratio_scorer,\n",
    "    thresholds=valid_thresholds,\n",
    "    cv=10,\n",
    ")\n",
    "threshold_tuning.fit(X_train, y_train)\n",
    "forest_threshold_undersampled = threshold_tuning.best_threshold_\n",
    "print('Best threshold:', forest_threshold_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58218005-a107-4ee4-9985-f142ca42a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "boost_pipeline_undersampled = Pipeline(\n",
    "    [\n",
    "        ('transformer', transformer),\n",
    "        ('undersampler', NearMiss()),\n",
    "        ('model', AdaBoostClassifier(algorithm='SAMME')),\n",
    "    ]\n",
    ")\n",
    "parameters = {\n",
    "    'undersampler__version': [1, 2, 3],\n",
    "    'model__n_estimators': [5, 10, 15, 20],\n",
    "    'model__learning_rate': [0.4, 0.8, 1.2, 1.6, 2],\n",
    "    'model__estimator': [\n",
    "        DecisionTreeClassifier(\n",
    "            max_depth=1,\n",
    "            criterion=crit)\n",
    "        for crit in ['gini', 'entropy']\n",
    "    ],\n",
    "}\n",
    "parameter_tuning = RandomizedSearchCV(\n",
    "    boost_pipeline_undersampled,\n",
    "    parameters,\n",
    "    n_iter=50,\n",
    "    scoring='average_precision',\n",
    "    cv=10,\n",
    ")\n",
    "parameter_tuning.fit(X_train, y_train)\n",
    "boost_pipeline_undersampled = parameter_tuning.best_estimator_\n",
    "display(boost_pipeline_undersampled)\n",
    "\n",
    "threshold_tuning = TunedThresholdClassifierCV(\n",
    "    boost_pipeline_undersampled,\n",
    "    scoring=diagnostic_odds_ratio_scorer,\n",
    "    thresholds=valid_thresholds,\n",
    "    cv=10,\n",
    ")\n",
    "threshold_tuning.fit(X_train, y_train)\n",
    "boost_threshold_undersampled = threshold_tuning.best_threshold_\n",
    "print('Best threshold:', boost_threshold_undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbf068c-4fe9-4b9b-a6f6-1a73114bac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ColumnTransformer(\n",
    "    [\n",
    "        ('num', StandardScaler(), numeric_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "svm_pipeline_undersampled = Pipeline(\n",
    "    [\n",
    "        ('transformer', transformer),\n",
    "        ('undersampler', NearMiss()),\n",
    "        ('model', SVC()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = [\n",
    "    {\n",
    "        'undersampler__version': [1, 2, 3],\n",
    "        'model__kernel': ['linear'],\n",
    "        'model__C': [2 ** (i - 9) for i in range(19)],\n",
    "    },\n",
    "    {\n",
    "        'undersampler__version': [1, 2, 3],\n",
    "        'model__kernel': ['poly'],\n",
    "        'model__C': [2 ** (i - 9) for i in range(10)],\n",
    "        'model__degree': [i + 2 for i in range(4)],\n",
    "        'model__coef0': [0, 0.1, 0.5, 1, 2, 10],\n",
    "    },\n",
    "    {\n",
    "        'undersampler__version': [1, 2, 3],\n",
    "        'model__kernel': ['rbf'],\n",
    "        'model__C': [2 ** (i - 9) for i in range(10)],\n",
    "        'model__gamma': [2 ** (i - 8) for i in range(17)],\n",
    "    },\n",
    "]\n",
    "parameter_tuning = RandomizedSearchCV(\n",
    "    svm_pipeline_undersampled,\n",
    "    parameters,\n",
    "    n_iter=50,\n",
    "    scoring=diagnostic_odds_ratio_scorer,\n",
    "    cv=10,\n",
    ")\n",
    "parameter_tuning.fit(X_train, y_train)\n",
    "svm_pipeline_undersampled = parameter_tuning.best_estimator_\n",
    "display(svm_pipeline_undersampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b989832-a98c-4dc9-9477-010ea21735a3",
   "metadata": {},
   "source": [
    "### Oversampling\n",
    "We will use SMOTENC oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a12278-5607-4480-9a6d-5277658c75e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols_count = len(numeric_cols)\n",
    "total_cols_count = len(X_train.columns)\n",
    "categorical_list = [i for i in range(numeric_cols_count, total_cols_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba56375-c41b-4727-b115-4ca6e398b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_numeric = ColumnTransformer(\n",
    "    [('num', StandardScaler(), numeric_cols)],\n",
    "    remainder='passthrough',\n",
    "    force_int_remainder_cols=False,\n",
    ")\n",
    "transformer_categorical = ColumnTransformer(\n",
    "    [('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_list)],\n",
    "    remainder='passthrough',\n",
    "    force_int_remainder_cols=False,\n",
    ")\n",
    "logistic_pipeline_oversampled = Pipeline(\n",
    "    [\n",
    "        ('transformer_numeric', transformer_numeric),\n",
    "        ('oversampler', SMOTENC(categorical_list)),\n",
    "        ('transformer_categorical', transformer_categorical),\n",
    "        ('model', LogisticRegression(penalty=None)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "threshold_tuning = TunedThresholdClassifierCV(\n",
    "    logistic_pipeline_oversampled,\n",
    "    scoring=diagnostic_odds_ratio_scorer,\n",
    "    thresholds=valid_thresholds,\n",
    "    cv=10,\n",
    ")\n",
    "threshold_tuning.fit(X_train, y_train)\n",
    "logistic_pipeline_oversampled = threshold_tuning.estimator_\n",
    "logistic_threshold_oversampled = threshold_tuning.best_threshold_\n",
    "display(logistic_pipeline_oversampled)\n",
    "print('Best threshold:', logistic_threshold_oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b281c5-355e-4721-a277-1b8160059afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_numeric = ColumnTransformer(\n",
    "    [('num', StandardScaler(), numeric_cols)],\n",
    "    remainder='passthrough',\n",
    "    force_int_remainder_cols=False,\n",
    ")\n",
    "transformer_categorical = ColumnTransformer(\n",
    "    [('cat', OneHotEncoder(handle_unknown='ignore'), categorical_list)],\n",
    "    remainder='passthrough',\n",
    "    force_int_remainder_cols=False,\n",
    ")\n",
    "forest_pipeline_oversampled = Pipeline(\n",
    "    [\n",
    "        ('transformer_numeric', transformer_numeric),\n",
    "        ('oversampler', SMOTENC(categorical_list)),\n",
    "        ('transformer_categorical', transformer_categorical),\n",
    "        ('model', RandomForestClassifier()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    'model__n_estimators': [100, 150, 200, 250],\n",
    "    'model__criterion': ['gini', 'entropy'],\n",
    "    'model__max_features': [0.1, 0.2, 0.3, 0.4],\n",
    "    'model__max_depth': [2, 3, 5],\n",
    "}\n",
    "parameter_tuning = RandomizedSearchCV(\n",
    "    forest_pipeline_oversampled,\n",
    "    parameters,\n",
    "    n_iter=50,\n",
    "    scoring='average_precision',\n",
    "    cv=10,\n",
    ")\n",
    "parameter_tuning.fit(X_train, y_train)\n",
    "forest_pipeline_oversampled = parameter_tuning.best_estimator_\n",
    "display(forest_pipeline_oversampled)\n",
    "\n",
    "threshold_tuning = TunedThresholdClassifierCV(\n",
    "    forest_pipeline_oversampled,\n",
    "    scoring=diagnostic_odds_ratio_scorer,\n",
    "    thresholds=valid_thresholds,\n",
    "    cv=10,\n",
    ")\n",
    "threshold_tuning.fit(X_train, y_train)\n",
    "forest_threshold_oversampled = threshold_tuning.best_threshold_\n",
    "print('Best threshold:', forest_threshold_oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f144696-18ea-408c-bdbb-4ec79abc54d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_numeric = ColumnTransformer(\n",
    "    [('num', StandardScaler(), numeric_cols)],\n",
    "    remainder='passthrough',\n",
    "    force_int_remainder_cols=False,\n",
    ")\n",
    "transformer_categorical = ColumnTransformer(\n",
    "    [('cat', OneHotEncoder(handle_unknown='ignore'), categorical_list)],\n",
    "    remainder='passthrough',\n",
    "    force_int_remainder_cols=False,\n",
    ")\n",
    "\n",
    "boost_pipeline_oversampled = Pipeline(\n",
    "    [\n",
    "        ('transformer_numeric', transformer_numeric),\n",
    "        ('oversampler', SMOTENC(categorical_list)),\n",
    "        ('transformer_categorical', transformer_categorical),\n",
    "        ('model', AdaBoostClassifier(algorithm='SAMME')),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    'model__n_estimators': [5, 10, 15, 20],\n",
    "    'model__learning_rate': [0.4, 0.8, 1.2, 1.6, 2],\n",
    "    'model__estimator': [\n",
    "        DecisionTreeClassifier(\n",
    "            max_depth=1,\n",
    "            criterion=crit)\n",
    "        for crit in ['gini', 'entropy']\n",
    "    ],\n",
    "}\n",
    "parameter_tuning = GridSearchCV(\n",
    "    boost_pipeline_oversampled,\n",
    "    parameters,\n",
    "    scoring='average_precision',\n",
    "    cv=10,\n",
    ")\n",
    "parameter_tuning.fit(X_train, y_train)\n",
    "boost_pipeline_oversampled = parameter_tuning.best_estimator_\n",
    "display(boost_pipeline_oversampled)\n",
    "\n",
    "threshold_tuning = TunedThresholdClassifierCV(\n",
    "    boost_pipeline_oversampled,\n",
    "    scoring=diagnostic_odds_ratio_scorer,\n",
    "    thresholds=valid_thresholds,\n",
    "    cv=10,\n",
    ")\n",
    "threshold_tuning.fit(X_train, y_train)\n",
    "boost_threshold_oversampled = threshold_tuning.best_threshold_\n",
    "print('Best threshold:', boost_threshold_oversampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4df521-b976-4227-9c2c-2f973fe8c03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_numeric = ColumnTransformer(\n",
    "    [('num', StandardScaler(), numeric_cols)],\n",
    "    remainder='passthrough',\n",
    "    force_int_remainder_cols=False,\n",
    ")\n",
    "transformer_categorical = ColumnTransformer(\n",
    "    [('cat', OneHotEncoder(handle_unknown='ignore'), categorical_list)],\n",
    "    remainder='passthrough',\n",
    "    force_int_remainder_cols=False,\n",
    ")\n",
    "\n",
    "svm_pipeline_oversampled = Pipeline(\n",
    "    [\n",
    "        ('transformer_numeric', transformer_numeric),\n",
    "        ('oversampler', SMOTENC(categorical_list)),\n",
    "        ('transformer_categorical', transformer_categorical),\n",
    "        ('model', SVC()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = [\n",
    "    {\n",
    "        'model__kernel': ['linear'],\n",
    "        'model__C': [2 ** (i - 9) for i in range(19)],\n",
    "    },\n",
    "    {\n",
    "        'model__kernel': ['poly'],\n",
    "        'model__C': [2 ** (i - 9) for i in range(10)],\n",
    "        'model__degree': [i + 2 for i in range(4)],\n",
    "        'model__coef0': [0, 0.1, 0.5, 1, 2, 10],\n",
    "    },\n",
    "    {\n",
    "        'model__kernel': ['rbf'],\n",
    "        'model__C': [2 ** (i - 9) for i in range(10)],\n",
    "        'model__gamma': [2 ** (i - 8) for i in range(17)],\n",
    "    },\n",
    "]\n",
    "parameter_tuning = RandomizedSearchCV(\n",
    "    svm_pipeline_oversampled,\n",
    "    parameters,\n",
    "    n_iter=50,\n",
    "    scoring=diagnostic_odds_ratio_scorer,\n",
    "    cv=10,\n",
    ")\n",
    "parameter_tuning.fit(X_train, y_train)\n",
    "svm_pipeline_oversampled = parameter_tuning.best_estimator_\n",
    "display(svm_pipeline_oversampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07c55f7-bd06-4a5c-b57f-2a741fb93e43",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c1c8f-8a23-4a2e-8532-d69f62d4e381",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data_test = pd.DataFrame(\n",
    "    columns=['regular', 'weighted', 'undersampled', 'oversampled']\n",
    ")\n",
    "summary_data_test.loc['logistic_regression', 'regular'] = diagnostic_odds_ratio(\n",
    "    y_test,\n",
    "    logistic_pipeline.predict_proba(X_test)[:, 1]\n",
    "    >= logistic_threshold,\n",
    ")\n",
    "summary_data_test.loc['logistic_regression', 'weighted'] = diagnostic_odds_ratio(\n",
    "    y_test,\n",
    "    logistic_pipeline_weighted.predict_proba(X_test)[:, 1]\n",
    "    >= logistic_threshold_weighted,\n",
    ")\n",
    "summary_data_test.loc['logistic_regression', 'undersampled'] = diagnostic_odds_ratio(\n",
    "    y_test,\n",
    "    logistic_pipeline_undersampled.predict_proba(X_test)[:, 1]\n",
    "    >= logistic_threshold_undersampled, \n",
    ")\n",
    "summary_data_test.loc['logistic_regression', 'oversampled'] = diagnostic_odds_ratio(\n",
    "    y_test,\n",
    "    logistic_pipeline_oversampled.predict_proba(X_test)[:, 1]\n",
    "    >= logistic_threshold_oversampled, \n",
    ")\n",
    "summary_data_test.loc['random_forest', 'regular'] = diagnostic_odds_ratio(\n",
    "    y_test,\n",
    "    forest_pipeline.predict_proba(X_test)[:, 1]\n",
    "    >= forest_threshold, \n",
    ")\n",
    "summary_data_test.loc['random_forest', 'weighted'] = diagnostic_odds_ratio(\n",
    "    y_test,\n",
    "    forest_pipeline_weighted.predict_proba(X_test)[:, 1]\n",
    "    >= forest_threshold_weighted, \n",
    ")\n",
    "summary_data_test.loc['random_forest', 'undersampled'] = diagnostic_odds_ratio(\n",
    "    y_test,\n",
    "    forest_pipeline_undersampled.predict_proba(X_test)[:, 1]\n",
    "    >= forest_threshold_undersampled, \n",
    ")\n",
    "summary_data_test.loc['random_forest', 'oversampled'] = diagnostic_odds_ratio(\n",
    "    y_test,\n",
    "    forest_pipeline_oversampled.predict_proba(X_test)[:, 1]\n",
    "    >= forest_threshold_oversampled, \n",
    ")\n",
    "summary_data_test.loc['boosting', 'regular'] = diagnostic_odds_ratio(\n",
    "    y_test,\n",
    "    boost_pipeline.predict_proba(X_test)[:, 1]\n",
    "    >= boost_threshold, \n",
    ")\n",
    "summary_data_test.loc['boosting', 'weighted'] = diagnostic_odds_ratio(\n",
    "    y_test,\n",
    "    boost_pipeline_weighted.predict_proba(X_test)[:, 1]\n",
    "    >= boost_threshold_weighted, \n",
    ")\n",
    "summary_data_test.loc['boosting', 'undersampled'] = diagnostic_odds_ratio(\n",
    "    y_test,\n",
    "    boost_pipeline_undersampled.predict_proba(X_test)[:, 1]\n",
    "    >= boost_threshold_undersampled,\n",
    ")\n",
    "summary_data_test.loc['boosting', 'oversampled'] = diagnostic_odds_ratio(\n",
    "    y_test,\n",
    "    boost_pipeline_oversampled.predict_proba(X_test)[:, 1]\n",
    "    >= boost_threshold_oversampled,\n",
    ")\n",
    "summary_data_test.loc['svm', 'regular'] = diagnostic_odds_ratio(\n",
    "    y_test,\n",
    "    svm_pipeline_weighted.predict(X_test),\n",
    ")\n",
    "summary_data_test.loc['svm', 'weighted'] = diagnostic_odds_ratio(\n",
    "    y_test,\n",
    "    svm_pipeline.predict(X_test),\n",
    ")\n",
    "summary_data_test.loc['svm', 'undersampled'] = diagnostic_odds_ratio(\n",
    "    y_test,\n",
    "    svm_pipeline_undersampled.predict(X_test),\n",
    ")\n",
    "summary_data_test.loc['svm', 'oversampled'] = diagnostic_odds_ratio(\n",
    "    y_test,\n",
    "    svm_pipeline_oversampled.predict(X_test),\n",
    ")\n",
    "summary_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e951a33-4d4d-4652-8038-9ff1923501cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data_train = pd.DataFrame(\n",
    "    columns=['regular', 'weighted', 'undersampled', 'oversampled']\n",
    ")\n",
    "summary_data_train.loc['logistic_regression', 'regular'] = diagnostic_odds_ratio(\n",
    "    y_train,\n",
    "    logistic_pipeline.predict_proba(X_train)[:, 1]\n",
    "    >= logistic_threshold,\n",
    ")\n",
    "summary_data_train.loc['logistic_regression', 'weighted'] = diagnostic_odds_ratio(\n",
    "    y_train,\n",
    "    logistic_pipeline_weighted.predict_proba(X_train)[:, 1]\n",
    "    >= logistic_threshold_weighted,\n",
    ")\n",
    "summary_data_train.loc['logistic_regression', 'undersampled'] = diagnostic_odds_ratio(\n",
    "    y_train,\n",
    "    logistic_pipeline_undersampled.predict_proba(X_train)[:, 1]\n",
    "    >= logistic_threshold_undersampled,\n",
    ")\n",
    "summary_data_train.loc['logistic_regression', 'oversampled'] = diagnostic_odds_ratio(\n",
    "    y_train,\n",
    "    logistic_pipeline_oversampled.predict_proba(X_train)[:, 1]\n",
    "    >= logistic_threshold_oversampled,\n",
    ")\n",
    "summary_data_train.loc['random_forest', 'regular'] = diagnostic_odds_ratio(\n",
    "    y_train,\n",
    "    forest_pipeline.predict_proba(X_train)[:, 1]\n",
    "    >= forest_threshold,\n",
    ")\n",
    "summary_data_train.loc['random_forest', 'weighted'] = diagnostic_odds_ratio(\n",
    "    y_train,\n",
    "    forest_pipeline_weighted.predict_proba(X_train)[:, 1]\n",
    "    >= forest_threshold_weighted,\n",
    ")\n",
    "summary_data_train.loc['random_forest', 'undersampled'] = diagnostic_odds_ratio(\n",
    "    y_train,\n",
    "    forest_pipeline_undersampled.predict_proba(X_train)[:, 1]\n",
    "    >= forest_threshold_undersampled,\n",
    ")\n",
    "summary_data_train.loc['random_forest', 'oversampled'] = diagnostic_odds_ratio(\n",
    "    y_train,\n",
    "    forest_pipeline_oversampled.predict_proba(X_train)[:, 1]\n",
    "    >= forest_threshold_oversampled,\n",
    ")\n",
    "summary_data_train.loc['boosting', 'regular'] = diagnostic_odds_ratio(\n",
    "    y_train,\n",
    "    boost_pipeline.predict_proba(X_train)[:, 1]\n",
    "    >= boost_threshold,\n",
    ")\n",
    "summary_data_train.loc['boosting', 'weighted'] = diagnostic_odds_ratio(\n",
    "    y_train,\n",
    "    boost_pipeline_weighted.predict_proba(X_train)[:, 1]\n",
    "    >= boost_threshold_weighted,\n",
    ")\n",
    "summary_data_train.loc['boosting', 'undersampled'] = diagnostic_odds_ratio(\n",
    "    y_train,\n",
    "    boost_pipeline_undersampled.predict_proba(X_train)[:, 1]\n",
    "    >= boost_threshold_undersampled,\n",
    ")\n",
    "summary_data_train.loc['boosting', 'oversampled'] = diagnostic_odds_ratio(\n",
    "    y_train,\n",
    "    boost_pipeline_oversampled.predict_proba(X_train)[:, 1]\n",
    "    >= boost_threshold_oversampled,\n",
    ")\n",
    "summary_data_train.loc['svm', 'regular'] = diagnostic_odds_ratio(\n",
    "    y_train,\n",
    "    svm_pipeline.predict(X_train),\n",
    ")\n",
    "summary_data_train.loc['svm', 'weighted'] = diagnostic_odds_ratio(\n",
    "    y_train,\n",
    "    svm_pipeline_weighted.predict(X_train),\n",
    ")\n",
    "summary_data_train.loc['svm', 'undersampled'] = diagnostic_odds_ratio(\n",
    "    y_train,\n",
    "    svm_pipeline_undersampled.predict(X_train),\n",
    ")\n",
    "summary_data_train.loc['svm', 'oversampled'] = diagnostic_odds_ratio(\n",
    "    y_train,\n",
    "    svm_pipeline_oversampled.predict(X_train),\n",
    ")\n",
    "summary_data_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41383d61-b15a-40b8-8253-1a896a892e84",
   "metadata": {},
   "source": [
    "It seems that random forest without any modifications is the way to go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02060a28-0170-4900-80dc-fecde85f3e74",
   "metadata": {},
   "source": [
    "### Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c4a1fa-a5ef-49c0-bb30-121721477838",
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = \\\n",
    "confusion_matrix(\n",
    "    y_test,\n",
    "    forest_pipeline.predict_proba(X_test)[:, 1] >= forest_threshold,\n",
    ").ravel()\n",
    "print(\n",
    "    'Accuracy:', (tp + tn) / (tp + fp + tn + fn),\n",
    "    'Sensitivity:', tp / (tp + fn),\n",
    "    'Specificity:', tn / (tn + fp),\n",
    "    'Precision:', tp / (tp + fp),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6099e542-312d-462d-9ad4-b2b062ab6147",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = permutation_importance(\n",
    "    forest_pipeline,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    n_repeats=1000,\n",
    ")\n",
    "importance_series = pd.Series(\n",
    "    feature_importance.importances_mean,\n",
    "    index=X_test.columns,\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "importance_series.plot.bar(yerr=feature_importance.importances_std, ax=ax)\n",
    "ax.set_title('Feature importances using MDI')\n",
    "ax.set_ylabel('Mean decrease in impurity')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adc4ab1-d7a2-4516-9b9b-5ee86d67d02b",
   "metadata": {},
   "source": [
    "It seems that only *Sex*, *FastingBS*, *Oldpeak* and *ST_Slope* may be considered important. This variables will be focused on in model interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec443b7-5544-4d46-932d-c43d2ac50159",
   "metadata": {},
   "source": [
    "### Model Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512946a6-0893-43fe-a8c7-87c1f04a3357",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = X_test.iloc[[100]]\n",
    "display(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978dd33b-dfaf-4f43-86e0-b0b1d33c945e",
   "metadata": {},
   "source": [
    "#### Ceteris Paribus (CP) Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1bb096-25e6-4aa7-bd53-caa06a57bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = dx.Explainer(\n",
    "    forest_pipeline,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    label='Random Forest',\n",
    "    verbose=0,\n",
    ")\n",
    "pcp = explainer.predict_profile(observation)\n",
    "pcp.plot()\n",
    "pcp.plot(variable_type='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b287a1-ff26-4d21-9840-59b632e36332",
   "metadata": {},
   "source": [
    "#### Partial Dependence (PD) Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a58bc02-2d53-4575-8322-38554c8f6518",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp = explainer.model_profile()\n",
    "pdp.plot()\n",
    "pdp.plot(geom='profiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420aba8c-e748-427b-a39f-1a50e076fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp = explainer.model_profile(variable_type='categorical')\n",
    "pdp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be71239d-607f-4980-bb94-6f499c01f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdp = explainer.model_profile(groups='Sex', variables=['Oldpeak'])\n",
    "pdp.plot(geom='profiles')\n",
    "pdp = explainer.model_profile(groups='FastingBS', variables=['Oldpeak'])\n",
    "pdp.plot(geom='profiles')\n",
    "pdp = explainer.model_profile(groups='ST_Slope', variables=['Oldpeak'])\n",
    "pdp.plot(geom='profiles')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b71a89f-61c1-4d19-89ba-7906360fcfdb",
   "metadata": {},
   "source": [
    "#### Break-down Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44502bc6-3003-42ad-8dc2-383b7e22695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bd = explainer.predict_parts(\n",
    "    observation,\n",
    "    type='break_down_interactions',\n",
    "    order=np.array(['Oldpeak', 'Sex', 'FastingBS', 'ST_Slope']),\n",
    ")\n",
    "bd.plot()\n",
    "bd = explainer.predict_parts(\n",
    "    observation,\n",
    "    type='break_down_interactions',\n",
    "    order=np.array(['Sex', 'FastingBS', 'ST_Slope', 'Oldpeak']),\n",
    ")\n",
    "bd.plot()\n",
    "bd = explainer.predict_parts(\n",
    "    observation,\n",
    "    type='break_down_interactions',\n",
    "    order=np.array(['Oldpeak', 'ST_Slope', 'Sex', 'FastingBS']),\n",
    ")\n",
    "bd.plot()\n",
    "bd = explainer.predict_parts(\n",
    "    observation,\n",
    "    type='break_down_interactions',\n",
    "    order=np.array(['ST_Slope', 'Sex', 'FastingBS', 'Oldpeak']),\n",
    ")\n",
    "bd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b8220-69ac-42ff-a32a-76996081b420",
   "metadata": {},
   "source": [
    "#### SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3049312c-78c4-4abb-834f-3fea57365f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap = explainer.predict_parts(observation, type='shap')\n",
    "shap.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
